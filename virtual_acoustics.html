<!--
* Skeleton V2.0.4
* Copyright 2014, Dave Gamache
* www.getskeleton.com
* Free to use under the MIT license.
* http://www.opensource.org/licenses/mit-license.php
* 12/29/2014
* Modified by Chad McKell
-->

<!DOCTYPE html>
<html>
<head>
  
   <!-- Basic Page Needs -->
  <meta charset="utf-8">
  <title>Virtual Acoustics</title>
  <meta name="description" content="">
  <meta name="author" content="">

  <!-- Mobile Specific Metas -->
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <!-- FONT -->
  <link href="//fonts.googleapis.com/css?family=Raleway:400,300,600" rel="stylesheet" type="text/css">

  <!-- CSS -->
  <link rel="stylesheet" href="css/custom.css">
  <link rel="stylesheet" href="css/normalize.css">
  <link rel="stylesheet" href="css/skeleton.css">

  <!-- Icons -->
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
  <link rel="icon" type="image/png" href="images/fairy2.jpg">
  
  <script type="text/javascript" async
    src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML" async>
  </script>
  

  <style>
    a {
      text-decoration: none;
      color: rgb(61, 68, 200); 
    } 
    </style>

</head>
<body>


  <header class="header">
    <a href="index" class="logo">CHAD MCKELL</a>
    <input class="menu-btn" type="checkbox" id="menu-btn" />
    <label class="menu-icon" for="menu-btn"><span class="navicon"></span></label>
    <ul class="menu">
      <li><a href="research">Research</a></li>
      <li><a href="publications">Publications</a></li>
      <!-- <li><a href="portfolio">Portfolio</a></li> -->
      <li><a href="teaching">Teaching</a></li>
      <li><a href="about">About</a></li>
    </ul>
  </header>
  
    
  <!-- <header>
    <div class="nav">
        
    <a id="menu-icon"></a>
        
      <ul>
          <li><a href="index">Home</a></li>
          <li><a href="research">Research</a></li>
          <li><a href="publications">Publications</a></li>
          <li><a href="teaching">Teaching</a></li>
          <li><a href="about">About</a></li>
      </ul>
      
    </div>
  </header> -->

  <div class="banner">
	  <img style="width:100%;" id="image" src="images/bunnies.jpeg">
  </div>

  <div class="section">
    <div class="container">
     
     
 
    <br>
    <h3><b>Virtual Acoustics</b></h3>

    <h5><b>Binaural Audio</b></h5>
    
      <p> 
        <div style="LINE-HEIGHT:18px; text-align:left"><font color="grey"> 
          Binaural audio technologies create three-dimensional sounds using two-channel sound 
          systems such as headphones. To produce virtual sounds  
          around a listener's head, we can record sounds 
          using microphones placed in the left and right ears of a 
          head and then play back the recording through headphones. However, this approach limits 
          the binauralization to the particular head and environment used in the recording. Alternatively, 
          we can simulate how non-spatialized, free-field sounds are 
          filtered by a human head and torso and then play that 
          filtered signal through headphones. This binaural filtering function is 
          known as the head-related transfer function (HRTF) and it can be used to binauralize sound for 
          any listener regardless of environment.

          In my research, I focus on modeling boundary conditions for HRTF calculations. 
          Of particular interest are differential geometric methods that accurately and efficiently 
          model open and closed boundaries encountered in HRTF simulations.
      </font></div>
      
      <br>
     
      <img src="images/hrtf.png" style="width:50%">
      </p>

      <br>

     



      <h5><b>Environmental Acoustics</b></h5>
          <p> 
            <div style="LINE-HEIGHT:18px; text-align:left"><font color="grey">
              Enviromental acoustics technologies incorporate the sound properties of 
        a physical space, such as a concert hall or amphitheater, into an audio signal. To incorporate
        these sound properties into an audio signal, we can convolve the signal with the impulse response 
        of the physical space. In my research, I build environmental acoustics simulations in order to 
        accurately and efficienly compute the impulse response of any space. The focus of my PhD dissertation 
        is on improving boundary condition modeling in these simulations.
            
            <!-- I modeled room reverberation using the image-source method. The model simulates the impulse response of a
            room and then convolves the impulse response with the original input audio in the time domain. For simplicity,
            I assumed that the walls of the room were flat and positioned perpendicular to each other.
            
            <br> <br>
            
            The equation that defines the magnitude \(g\) of each impulse in the simulated impulse response takes the form
            
            $$g = {(\sqrt{1-\alpha})^w \over l}$$
            
            where \(\alpha\) is the absorption coefficient of the walls, \(w\) is the total number of collisions between the 
            sound wave and the walls for each image source, and \(l\) is the total distance between the receiver and each 
            image source. (Note that high absorption values correspond to low reverberant rooms). The MATLAB 
            <a href="https://github.com/chadmckell/ImageSourceBasic" target="_blank"><b>code</b></a> for my reverb model is available on my GitHub
            page.
          </font></div>
          </p>

        <table style='border:none;'>
          <tr>
            <td>Dry</td>
            <td>
              <audio controls>
                <source src="audio/guitar_dry.wav" type="audio/wav">
              </audio>
            </td>
          </tr>
          <tr>
            <td>High Absorption</td> 
            <td>
              <audio controls>
                <source src="audio/reverb_high.wav" type="audio/wav">
              </audio>
            </td>
          </tr>
          <tr>
            <td>Low Absorption</td>
            <td>
              <audio controls>
                <source src="audio/reverb_low.wav" type="audio/wav">
              </audio>
            </td>
          </tr>
        </table> -->
  

  

         
    


      
         
          <br><br>
          
      </p>

    </div>
  </div>


<!--
<div class="section">
    <div class="container">
      <div class="one-half column">
        <br>
        <h4><b>Sound Synthesis for Animation</b></h4>
        <h5><b>String Synthesis</b></h5>
          <p> 
            <div style="LINE-HEIGHT:16px; text-align:left"><font color="grey">Strings are the basic building blocks of many musical instruments, including guitars, violins, and dulcimers.
            Various physical modeling schemes exist for modeling strings, such as the Karplus-Strong technique,
            modal synthesis, and finite-difference time-domain (FDTD) methods. The displacement \(u = u(x, t)\) of a lossy 
            forced string of finite length \(L\) and fixed boundary conditions is described by the 1D wave equation
            with loss and forcing 
            
            $$u_{tt} = c^2 u_{xx} - 2\sigma_0 u_t + \delta F$$

            where \(c\) is the speed of sound in the string, \(\sigma_0\) is a non-negative damping constant, 
            \(\delta = \delta(x-x_i)\) is the 1D delta function centered at the excitation location \(x_i\), and \(F\) is the 
            time-dependent forcing signal.
            
            <br> <br>
            
            I modeled string vibrations by solving for the displacement \(u\) using FDTD and modal synthesis. For 
            comparison, I also simulated string vibrations using the Karplus-Strong (KS) approach, which 
            implements delays to approximate the vibrations.
          </font></div>
          </p>
      </div>
      <div class="one-half column">
        <br> <br> <br><br>
        <img src="images/string.png" style="width:85%">
        <video width="500" height="300" loop autoplay>
        <source src="video/string.mp4" type="video/mp4">
        </video>
        <table>
          <tr>
            <td>FDTD</td>
            <td>
              <audio controls>
                <source src="audio/string_fdtd.wav" type="audio/wav">
              </audio>
            </td>
          </tr>
          <tr>
            <td>Modal</td>
            <td>
              <audio controls>
                <source src="audio/string_modal.wav" type="audio/wav">
              </audio>
            </td>
          </tr>
          <tr>
            <td>KS</td>
            <td>
              <audio controls>
                <source src="audio/string_ks.wav" type="audio/wav">
              </audio>
            </td>
          </tr>
        </table>
      </div>

    </div>
  </div>
  
  
    <div class="section">
    <div class="container">
      <div class="one-half column">
        <h5><b>Membrane Synthesis</b></h5>
          <p> 
            <div style="LINE-HEIGHT:16px; text-align:left"><font color="grey">Percussion instruments like kick drums and toms are made of membranes. The displacement \(u = u(x,y,t)\) of a lossy 
            forced rectangular membrane of finite dimensions \(L_x \times L_y\) and fixed boundary conditions is described by
            the 2D wave equation with loss and forcing
            
            $$u_{tt} = c^2 \Delta u - 2\sigma_0 u_t + \delta F$$
            
            where \(c\) is the speed of sound in the membrane, \(\sigma_0\) is a non-negative damping constant, 
            \(\delta = \delta(x-x_i, y-y_i)\) is the 2D delta function centered at the excitation location \((x_i, y_i)\), 
            and \(F\) is the time-dependent forcing signal. I used modal synthesis to simulate vibrations from a membrane.</font></div>
          </p>
      </div>
      <div class="one-half column">
        <br> <br> 
        <img src="images/membrane.jpg" style="width:85%">
        <video width="500" height="300" loop autoplay>
        <source src="video/membrane.mp4" type="video/mp4">
        </video>
        <table>
          <tr>
            <td>Membrane</td>
            <td>
              <audio controls>
                <source src="audio/membrane.wav" type="audio/wav">
              </audio>
            </td>
          </tr>
        </table>
      </div>
    </div>
  </div>
  
  
  
  <div class="section">
    <div class="container">
      <div class="one-half column">
        <h5><b>Plate Synthesis</b></h5>
          <p> 
            <div style="LINE-HEIGHT:16px; text-align:left"><font color="grey">Many musical instruments are made of plates. For example, the soundboard of a piano and the fretboard of a guitar
            can both be modeled as rectangular plates. The displacement \(u = u(x,y,t)\) of a lossy forced rectangular plate 
            of finite dimensions \(L_x \times L_y\) and fixed boundary conditions is described by the Kirchhoff thin plate 
            equation with loss and forcing
            
            $$u_{tt} = -\kappa^2 \Delta \Delta u - 2\sigma_0 u_t + \delta F$$
            
            where \(\kappa\) is the stiffness parameter, \(\sigma_0\) is a non-negative damping constant, 
            \(\delta = \delta(x-x_i, y-y_i)\) is the 2D delta function centered at the excitation location \((x_i, y_i)\), 
            and \(F\) is the time-dependent forcing signal.
            
            I used modal synthesis to simulate vibrations from plates made of different materials, such as metal and wood. 
            I also modeled different excitations, including impulses and scratches.</font></div>
          </p>
      </div>
      <div class="one-half column">
        <br> <br>
        <img src="images/plate.png" style="width:85%">
        <video width="500" height="300" loop autoplay>
        <source src="video/plate.mp4" type="video/mp4">
        </video>
        <table>
          <tr>
            <td>Metal</td>
            <td>
              <audio controls>
                <source src="audio/plate_metal.wav" type="audio/wav">
              </audio>
            </td>
          </tr>
          <tr>
            <td>Wood</td> 
            <td>
              <audio controls>
                <source src="audio/plate_wood.wav" type="audio/wav">
              </audio>
            </td>
          </tr>
        </table>
      </div>
    </div>
  </div>






  <div class="section">
    <div class="container">
      <div class="one-half column">
        <h5><b>Speech Synthesis</b></h5>
          <p> 
            <div style="LINE-HEIGHT:16px; text-align:left"><font color="grey">Wolfgang von Kempelen created the first known speech synthesizer in 1791. The device used a variety of parts to 
              imitate human speech&mdash;a bellows for the lungs, a reed for the vocal folds, tubes for the various vocal-tract geometries, 
              and so on. By reproducing the subtleties of linguistic sounds from observations of the acoustic and physiological 
              mechanisms of speech, Kempelen set the stage for more advanced speech synthesis techniques that would emerge 
              centuries later.
              
              <br> <br>
  
              Physical modeling speech synthesis is a computational approach to artificial voice production that generates 
              acoustic sounds by numerically solving a mathematical model of speech. As part of my special project 
              <a href="/pdf/speech_2017.pdf">dissertation</a> in Acoustics and Music Technology at the University of Edinburgh, 
              I developed physical modeling simulations of vocal-tract sound propagation by solving Webster's equation with
              finite-difference time-domain (FDTD) methods. For another course project, I also created a unit selection (US) speech 
              synthesizer, which concatenates individual diphones of speech. Code from my 
              <a href="https://github.com/chadmckell/FDTDSpeech">FDTD</a> and <a href="https://github.com/chadmckell/UnitSpeech">unit selection</a>
              speech synthesizers is available on my GitHub page.
              
              <br> <br>
              
              To demonstrate my simulations, I used the English phrase 
              
              <br><br>
             
              <center><i>I owe you a yo-yo.</i></center> 
              
              <br>
              
              I chose this phrase because it contains only vowels and diphthongs. Other speech sounds, like consonants
              and glottal fry, were reserved for future research.</font></div>
          </p>
      </div>

      <div class="one-half column">
        <br> <br> 
        <img src="images/speech.jpg" style="width:85%">
        <table>
          <tr>
            <td>FDTD</td>
            <td>
              <audio controls>
                <source src="audio/speech_fdtd_quiet.wav" type="audio/wav">
              </audio>
            </td>
          </tr>
          <tr>
            <td>US</td>
            <td>
              <audio controls>
                <source src="audio/speech_unit.wav" type="audio/wav">
              </audio>
            </td>
          </tr>
        </table>
      </div>
    </div>
  </div>
-->




<br>

<div class="footer"> 
  <a href="https://vimeo.com/mckell" target="_blank"><i class="fa fa-vimeo-square" style="font-size:36px"></i></a>
  <a href="https://github.com/chadmckell" target="_blank"><i class="fa fa-github" style="font-size:36px"></i> </a>
  <a href="https://www.linkedin.com/in/chadmckell/" target="_blank"><i class="fa fa-linkedin-square" style="font-size:36px"></i></a>
  <!-- <a href="https://brownian.bandcamp.com/releases" target="_blank"><i class="fa fa-bandcamp" style="font-size:36px"></i></a> -->
  <br> <br>
  <font color="white">Copyright &copy 2009&ndash;2020. Chad McKell. All Rights Reserved.</font>
</div>

</body>
</html>
