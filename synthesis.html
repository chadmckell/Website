<!--
* Skeleton V2.0.4
* Copyright 2014, Dave Gamache
* www.getskeleton.com
* Free to use under the MIT license.
* http://www.opensource.org/licenses/mit-license.php
* 12/29/2014
* Modified by Chad McKell
-->

<!DOCTYPE html>
<html>
<head>
  
   <!-- Basic Page Needs -->
  <meta charset="utf-8">
  <title>Sound</title>
  <meta name="description" content="">
  <meta name="author" content="">

  <!-- Mobile Specific Metas -->
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <!-- FONT -->
  <link href="//fonts.googleapis.com/css?family=Raleway:400,300,600" rel="stylesheet" type="text/css">

  <!-- CSS -->
  <link rel="stylesheet" href="css/custom.css">
  <link rel="stylesheet" href="css/normalize.css">
  <link rel="stylesheet" href="css/skeleton.css">

  <!-- Icons -->
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
  <link rel="icon" type="image/png" href="images/fairy2.jpg">
  
  <script type="text/javascript" async
    src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML" async>
  </script>
  

  <style>
    a {
      text-decoration: none;
      color: rgb(61, 68, 200); 
    } 
    </style>

</head>
<body>


  <header class="header">
    <a href="index" class="logo">CHAD MCKELL</a>
    <input class="menu-btn" type="checkbox" id="menu-btn" />
    <label class="menu-icon" for="menu-btn"><span class="navicon"></span></label>
    <ul class="menu">
      <li><a href="research">Research</a></li>
      <li><a href="publications">Publications</a></li>
      <li><a href="portfolio">Portfolio</a></li>
      <li><a href="teaching">Teaching</a></li>
      <li><a href="about">About</a></li>
    </ul>
  </header>
  
    
  <!-- <header>
    <div class="nav">
        
    <a id="menu-icon"></a>
        
      <ul>
          <li><a href="index">Home</a></li>
          <li><a href="research">Research</a></li>
          <li><a href="publications">Publications</a></li>
          <li><a href="teaching">Teaching</a></li>
          <li><a href="about">About</a></li>
      </ul>
      
    </div>
  </header> -->

  <div class="banner">
	  <img style="width:100%;" id="image" src="images/bunnies.jpeg">
  </div>

  <div class="section">
    <div class="container">
     
     
 
    <br>
    <h3><b>Physical Modeling Sound Synthesis</b></h3>

    <h5><b>Sound Source Simulation</b></h5>
    <h6><b><i>Complex Soft-Body Geometries</i></b></h6>

    <!-- <h6><b>Acoustic Simulation for Complex Geometries</b></h6> -->
      <p> 
        <div style="LINE-HEIGHT:18px; text-align:left"><font color="grey"> Below is an image of four of the harmonic modes of the Stanford 
            bunny. From left to right, I've plotted the \(0^{th}\), \(1^{st}\), \(2^{nd}\), and \(6^{th}\) modes. I computed the 
            displacement \(u\) of each mode by numerically solving the Helmholtz equation, \( \Delta u = -k^2 u \), at every vertex on 
            the triangulated surface. The purple portions represent regions where the displacement is maximally negative, and the red portions 
            represent where it is maximally positive. In discrete exterior calculus, 
            the Helmholtz equation takes the form

            $$(d^T \star_1 d) u = k^2 \star_0 u $$
            
            where \(d^T \star_1 d\) is the discrete Laplacian, \(\star_0\) is the mass matrix, and \(k\) is the wave number of the harmonic mode. Here, 
            \(d\) is a sparse matrix defined using the source and destination of each half edge on the mesh, \(\star_1\) 
            is a sparse diagonal matrix whose non-zero values are the half edge weights, and \(\star_0\) is a sparse diagonal matrix 
            whose non-zero values are the vertex weights. The computation was performed using VEX and Python, and the 
            graphics were rendered using Houdini. Note: if the bunnies were replaced by spheres, the 
            eigenvectors \(u\) would be the corresponding real spherical harmonics.

      </font></div>
      <!-- Rigid bodies&mdash;such as strings, membranes, and plates&mdash;are the basic building blocks of common acoustic systems 
        in computer animation, including musical instruments 

        Various physical modeling schemes exist for simulating these objects, including modal 
        synthesis and finite-difference time-domain (FDTD) methods. I simulated several categories of object vibrations by solving for 
        the displacement of a point located on the object over time using one of these methods.
      
      -->
      <br>
     
      <img src="images/bunnies_full.jpeg" style="width:100%">
      </p>

      <br>

      <h6><b><i>Musical Instrument Geometries</i></b></h6>
      <p> 
        <div style="LINE-HEIGHT:18px; text-align:left"><font color="grey">Strings, membranes, and plates are the basic building 
        blocks of musical instruments. Using modal synthesis and finite-difference time-domain (FDTD) methods, I simulated sounds  
        corresponding to each of these musical instrument objects. In the case of plates, I modeled different materials and 
        excitation mechanisms. Audio samples of each musical instrument object are provided below. For future work, I plan to 
        integrate the sounds into virtual reality. In preparation, I have graphically rendered musical instruments to use as 
        interaction models (see violin below).
      </font></div>
      <!-- Rigid bodies&mdash;such as strings, membranes, and plates&mdash;are the basic building blocks of common acoustic systems 
        in computer animation, including musical instruments 

        Various physical modeling schemes exist for simulating these objects, including modal 
        synthesis and finite-difference time-domain (FDTD) methods. I simulated several categories of object vibrations by solving for 
        the displacement of a point located on the object over time using one of these methods.
      
      -->
      <br>
      <table style='border:none;'>
        <tr>
          <td>String</td>
          <td>
            <audio controls>
              <source src="audio/string_fdtd.wav" type="audio/wav">
            </audio>
          </td>
        </tr>
        <tr>
          <td>Membrane</td>
          <td>
            <audio controls>
              <source src="audio/membrane.wav" type="audio/wav">
            </audio>
          </td>
        </tr>
          <tr>
            <td>Plate</td>
            <td>
              <audio controls>
                <source src="audio/plate_metal.wav" type="audio/wav">
              </audio>
            </td>
          </tr>
      </table>
      <img src="images/violin_render.png" style="width:100%">
      </p>

      <br>
      <hr>



      <h5><b>Wave Propagation Synthesis</b></h5>

      <h6><b><i>Room Acoustics</i></b></h6>
          <p> 
            <div style="LINE-HEIGHT:18px; text-align:left"><font color="grey">I modeled room reverberation using the image-source method. The model simulates the impulse response of a
            room and then convolves the impulse response with the original input audio in the time domain. For simplicity,
            I assumed that the walls of the room were flat and positioned perpendicular to each other.
            
            <br> <br>
            
            The equation that defines the magnitude \(g\) of each impulse in the simulated impulse response takes the form
            
            $$g = {(\sqrt{1-\alpha})^w \over l}$$
            
            where \(\alpha\) is the absorption coefficient of the walls, \(w\) is the total number of collisions between the 
            sound wave and the walls for each image source, and \(l\) is the total distance between the receiver and each 
            image source. (Note that high absorption values correspond to low reverberant rooms). The MATLAB 
            <a href="https://github.com/chadmckell/ImageSourceBasic" target="_blank"><b>code</b></a> for my reverb model is available on my GitHub
            page.
          </font></div>
          </p>

        <table style='border:none;'>
          <tr>
            <td>Dry</td>
            <td>
              <audio controls>
                <source src="audio/guitar_dry.wav" type="audio/wav">
              </audio>
            </td>
          </tr>
          <tr>
            <td>High Absorption</td> 
            <td>
              <audio controls>
                <source src="audio/reverb_high.wav" type="audio/wav">
              </audio>
            </td>
          </tr>
          <tr>
            <td>Low Absorption</td>
            <td>
              <audio controls>
                <source src="audio/reverb_low.wav" type="audio/wav">
              </audio>
            </td>
          </tr>
        </table>
  

      <h6><b><i>Vocal Tract Propagation</i></b></h6>
      <p> 
        <div style="LINE-HEIGHT:18px; text-align:left"><font color="grey">Wolfgang von Kempelen created the first known speech synthesizer in 1791. The device used a variety of parts to 
          imitate human speech&mdash;a bellows for the lungs, a reed for the vocal folds, tubes for the various vocal-tract geometries, 
          and so on. By reproducing the subtleties of linguistic sounds from observations of the acoustic and physiological 
          mechanisms of speech, Kempelen set the stage for more advanced speech synthesis techniques that would emerge 
          centuries later.
          
          <br> <br>

          Physical modeling speech synthesis is a computational approach to artificial voice production that generates 
          acoustic sounds by numerically solving a mathematical model of speech. As part of my special project 
          <a href="/pdf/speech_2017.pdf" target="_blank"><b>dissertation</b></a> in Acoustics and Music Technology at the University of Edinburgh, 
          I developed physical modeling simulations of vocal-tract sound propagation by solving Webster's equation with
          finite-difference time-domain (FDTD) methods. For another course project, I also created a unit selection speech 
          synthesizer, which concatenates individual diphones of speech. Code from my 
          <a href="https://github.com/chadmckell/FDTDSpeech" target="_blank"><b>FDTD</b></a> and <a href="https://github.com/chadmckell/UnitSpeech" target="_blank"><b>unit selection</b></a>
          speech synthesizers is available on my GitHub page. To demonstrate my simulations, I used the English phrase <i>I owe you a yo-yo</i>.
          I chose this phrase because it contains only vowels and diphthongs. Other speech sounds, like consonants
          and glottal fry, were left for future research. Similar to musical instrument synthesis discussed above, I plan to synchronize 
          speech sounds with facial animations.
          <br><br>
          <!-- Similar to musical instrument synthesis discussed above, I plan to synchronize speech sounds with facial animations. I have built 
          simple character rigs (like the rig of a dragon shown below) to use in this research. -->
        </font></div>
          <br>
          <table style='border:none;'>
            <tr>
              <td>FDTD Speech</td>
              <td>
                <audio controls>
                  <source src="audio/speech_fdtd_quiet.wav" type="audio/wav">
                </audio>
              </td>
            </tr>
            <tr>
              <td>Unit Selection</td>
              <td>
                <audio controls>
                  <source src="audio/speech_unit.wav" type="audio/wav">
                </audio>
              </td>
            </tr>
  
          </table>
  

          <!-- <img src="images/dragon_render.png" style="width:100%"> -->

          <!-- <br><br> -->
          <font color="grey">
          <h6><b>References</b></h6>
          <div style="LINE-HEIGHT:18px; text-align:left">
                 <ol>
                  <li><b>C. McKell</b>. <i>Finite-Difference Simulations of Speech with Wall Vibration Losses </i>, Special Project Master's Dissertation, University of Edinburgh, Acoustics and Audio Group, April 2017. [<a href="/pdf/speech_2017.pdf"  target="_blank"><b>paper</b></a>]</li> 
                </ol>
                </font></div>
      </p>

      <br>
      <hr>


      <h5><b>Sound Synthesis for Computer Animation</b></h5>
      <p> 
        <div style="LINE-HEIGHT:18px; text-align:left"><font color="grey"> The images, motion, and sounds of the animation below 
          were generated entirely by a computer. The images were computed at 60 Hz, the motion was computed at 240 Hz, and the 
          sounds were computed at 48,000 Hz. Each sound event consisted of a pure tone modified by an attack-decay-release (ASR) 
          volume envelope. From left to right in the animation, the spheres played frequencies of 220 Hz, 440 Hz, and 880 Hz. Following the main computation, 
          the audio and video were automatically combined using FFmpeg. The computation was performed using C++, and the 
          graphics were rendered using OpenGL. The main computation took the following general form:
          <br><br>

          
          for (int i = 0; i < 60; i++) {<br>
            &emsp;// render images of falling spheres at 60 Hz
            <br><br>
            &emsp; for (int j = 0; j < 4; j++) { <br> 
              &emsp; &emsp; // update sphere motion at 60 \(\times\) 4 = 240 Hz <br> <br>
              &emsp; &emsp; for (int k = 0; k < numberOfSpheres; k++) { <br>
              &emsp; &emsp; &emsp;// advance position of each sphere using forward Euler method <br>
              &emsp; &emsp; &emsp;// initialize new sound event if sphere collides with floor <br>
              &emsp; <br>

              &emsp; &emsp; &emsp; for (int w = 0; w < 200; w++) { <br>
                &emsp;&emsp; &emsp; &emsp; // compute audio samples for each sound event at 60 \(\times\) 4 \(\times\) 200 = 48,000 Hz <br>
                &emsp;&emsp; &emsp; }<br>
                &emsp;&emsp; }<br>
                &emsp; }<br>

          }
          <br> <br>
 
        </font></div>
        <br>
          

        <iframe class="u-max-full-width" src="https://player.vimeo.com/video/434509270" width="640" height="480" 
        frameborder="0" allow="autoplay; fullscreen" allowfullscreen></iframe>

          <br><br>
          
      </p>

    </div>
  </div>


<!--
<div class="section">
    <div class="container">
      <div class="one-half column">
        <br>
        <h4><b>Sound Synthesis for Animation</b></h4>
        <h5><b>String Synthesis</b></h5>
          <p> 
            <div style="LINE-HEIGHT:16px; text-align:left"><font color="grey">Strings are the basic building blocks of many musical instruments, including guitars, violins, and dulcimers.
            Various physical modeling schemes exist for modeling strings, such as the Karplus-Strong technique,
            modal synthesis, and finite-difference time-domain (FDTD) methods. The displacement \(u = u(x, t)\) of a lossy 
            forced string of finite length \(L\) and fixed boundary conditions is described by the 1D wave equation
            with loss and forcing 
            
            $$u_{tt} = c^2 u_{xx} - 2\sigma_0 u_t + \delta F$$

            where \(c\) is the speed of sound in the string, \(\sigma_0\) is a non-negative damping constant, 
            \(\delta = \delta(x-x_i)\) is the 1D delta function centered at the excitation location \(x_i\), and \(F\) is the 
            time-dependent forcing signal.
            
            <br> <br>
            
            I modeled string vibrations by solving for the displacement \(u\) using FDTD and modal synthesis. For 
            comparison, I also simulated string vibrations using the Karplus-Strong (KS) approach, which 
            implements delays to approximate the vibrations.
          </font></div>
          </p>
      </div>
      <div class="one-half column">
        <br> <br> <br><br>
        <img src="images/string.png" style="width:85%">
        <video width="500" height="300" loop autoplay>
        <source src="video/string.mp4" type="video/mp4">
        </video>
        <table>
          <tr>
            <td>FDTD</td>
            <td>
              <audio controls>
                <source src="audio/string_fdtd.wav" type="audio/wav">
              </audio>
            </td>
          </tr>
          <tr>
            <td>Modal</td>
            <td>
              <audio controls>
                <source src="audio/string_modal.wav" type="audio/wav">
              </audio>
            </td>
          </tr>
          <tr>
            <td>KS</td>
            <td>
              <audio controls>
                <source src="audio/string_ks.wav" type="audio/wav">
              </audio>
            </td>
          </tr>
        </table>
      </div>

    </div>
  </div>
  
  
    <div class="section">
    <div class="container">
      <div class="one-half column">
        <h5><b>Membrane Synthesis</b></h5>
          <p> 
            <div style="LINE-HEIGHT:16px; text-align:left"><font color="grey">Percussion instruments like kick drums and toms are made of membranes. The displacement \(u = u(x,y,t)\) of a lossy 
            forced rectangular membrane of finite dimensions \(L_x \times L_y\) and fixed boundary conditions is described by
            the 2D wave equation with loss and forcing
            
            $$u_{tt} = c^2 \Delta u - 2\sigma_0 u_t + \delta F$$
            
            where \(c\) is the speed of sound in the membrane, \(\sigma_0\) is a non-negative damping constant, 
            \(\delta = \delta(x-x_i, y-y_i)\) is the 2D delta function centered at the excitation location \((x_i, y_i)\), 
            and \(F\) is the time-dependent forcing signal. I used modal synthesis to simulate vibrations from a membrane.</font></div>
          </p>
      </div>
      <div class="one-half column">
        <br> <br> 
        <img src="images/membrane.jpg" style="width:85%">
        <video width="500" height="300" loop autoplay>
        <source src="video/membrane.mp4" type="video/mp4">
        </video>
        <table>
          <tr>
            <td>Membrane</td>
            <td>
              <audio controls>
                <source src="audio/membrane.wav" type="audio/wav">
              </audio>
            </td>
          </tr>
        </table>
      </div>
    </div>
  </div>
  
  
  
  <div class="section">
    <div class="container">
      <div class="one-half column">
        <h5><b>Plate Synthesis</b></h5>
          <p> 
            <div style="LINE-HEIGHT:16px; text-align:left"><font color="grey">Many musical instruments are made of plates. For example, the soundboard of a piano and the fretboard of a guitar
            can both be modeled as rectangular plates. The displacement \(u = u(x,y,t)\) of a lossy forced rectangular plate 
            of finite dimensions \(L_x \times L_y\) and fixed boundary conditions is described by the Kirchhoff thin plate 
            equation with loss and forcing
            
            $$u_{tt} = -\kappa^2 \Delta \Delta u - 2\sigma_0 u_t + \delta F$$
            
            where \(\kappa\) is the stiffness parameter, \(\sigma_0\) is a non-negative damping constant, 
            \(\delta = \delta(x-x_i, y-y_i)\) is the 2D delta function centered at the excitation location \((x_i, y_i)\), 
            and \(F\) is the time-dependent forcing signal.
            
            I used modal synthesis to simulate vibrations from plates made of different materials, such as metal and wood. 
            I also modeled different excitations, including impulses and scratches.</font></div>
          </p>
      </div>
      <div class="one-half column">
        <br> <br>
        <img src="images/plate.png" style="width:85%">
        <video width="500" height="300" loop autoplay>
        <source src="video/plate.mp4" type="video/mp4">
        </video>
        <table>
          <tr>
            <td>Metal</td>
            <td>
              <audio controls>
                <source src="audio/plate_metal.wav" type="audio/wav">
              </audio>
            </td>
          </tr>
          <tr>
            <td>Wood</td> 
            <td>
              <audio controls>
                <source src="audio/plate_wood.wav" type="audio/wav">
              </audio>
            </td>
          </tr>
        </table>
      </div>
    </div>
  </div>






  <div class="section">
    <div class="container">
      <div class="one-half column">
        <h5><b>Speech Synthesis</b></h5>
          <p> 
            <div style="LINE-HEIGHT:16px; text-align:left"><font color="grey">Wolfgang von Kempelen created the first known speech synthesizer in 1791. The device used a variety of parts to 
              imitate human speech&mdash;a bellows for the lungs, a reed for the vocal folds, tubes for the various vocal-tract geometries, 
              and so on. By reproducing the subtleties of linguistic sounds from observations of the acoustic and physiological 
              mechanisms of speech, Kempelen set the stage for more advanced speech synthesis techniques that would emerge 
              centuries later.
              
              <br> <br>
  
              Physical modeling speech synthesis is a computational approach to artificial voice production that generates 
              acoustic sounds by numerically solving a mathematical model of speech. As part of my special project 
              <a href="/pdf/speech_2017.pdf">dissertation</a> in Acoustics and Music Technology at the University of Edinburgh, 
              I developed physical modeling simulations of vocal-tract sound propagation by solving Webster's equation with
              finite-difference time-domain (FDTD) methods. For another course project, I also created a unit selection (US) speech 
              synthesizer, which concatenates individual diphones of speech. Code from my 
              <a href="https://github.com/chadmckell/FDTDSpeech">FDTD</a> and <a href="https://github.com/chadmckell/UnitSpeech">unit selection</a>
              speech synthesizers is available on my GitHub page.
              
              <br> <br>
              
              To demonstrate my simulations, I used the English phrase 
              
              <br><br>
             
              <center><i>I owe you a yo-yo.</i></center> 
              
              <br>
              
              I chose this phrase because it contains only vowels and diphthongs. Other speech sounds, like consonants
              and glottal fry, were reserved for future research.</font></div>
          </p>
      </div>

      <div class="one-half column">
        <br> <br> 
        <img src="images/speech.jpg" style="width:85%">
        <table>
          <tr>
            <td>FDTD</td>
            <td>
              <audio controls>
                <source src="audio/speech_fdtd_quiet.wav" type="audio/wav">
              </audio>
            </td>
          </tr>
          <tr>
            <td>US</td>
            <td>
              <audio controls>
                <source src="audio/speech_unit.wav" type="audio/wav">
              </audio>
            </td>
          </tr>
        </table>
      </div>
    </div>
  </div>
-->




<br>

<div class="footer"> 
  <a href="https://vimeo.com/mckell" target="_blank"><i class="fa fa-vimeo-square" style="font-size:36px"></i></a>
  <a href="https://github.com/chadmckell" target="_blank"><i class="fa fa-github" style="font-size:36px"></i> </a>
  <a href="https://www.linkedin.com/in/chadmckell/" target="_blank"><i class="fa fa-linkedin-square" style="font-size:36px"></i></a>
  <!-- <a href="https://brownian.bandcamp.com/releases" target="_blank"><i class="fa fa-bandcamp" style="font-size:36px"></i></a> -->
  <br> <br>
  <font color="white">Copyright &copy 2009&ndash;2020. Chad McKell. All Rights Reserved.</font>
</div>

</body>
</html>
